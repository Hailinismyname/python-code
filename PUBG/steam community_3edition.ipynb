from urllib.request import urlopen  
from urllib.parse import urlparse  
from bs4 import BeautifulSoup   
from urllib  import request  
import urllib  
import re
import datetime as dt
import pandas as pd
import csv

data_retrieve=[]
i=0

def load_page(url):

    page = request.Request(url)
    #proxy_support=urllib.request.ProxyHandler({'https':'10.172.49.230:3128'})
    opener=urllib.request.build_opener(urllib.request.HTTPHandler)
    urllib.request.install_opener(opener)
    page.add_header('User-Agent','Mozilla/5.0 (Windows NT 6.1; \
        WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36')

    html = request.urlopen(page).read()

    soup = BeautifulSoup(html,'html.parser')
    
    get_herf_link(soup)

def tieba_spider(Url,startPage,endPage):
    #saving the desired data in a CSV file.
    for i in range(startPage,endPage + 1):
        page = i 
        url = Url + str(page)
        load_page(url)
        print(url)
        print("--------Downloading page %d, please wait----------" %i)
    
    save_to_csv(data_retrieve)

def get_topic_name(html):
    if len(html.find_all('div', class_='forum_topic_pinned_notice forum_newtopic_box')) == 1:
        pass
    
    else:
        posting=html.find_all('div')
        for each in posting:
            if each.find(class_='topic') is not None:
                topic=each.find(class_='topic').get_text()

                #print(topic)
                return topic


def get_topic_content(html):
    if len(html.find_all('div', class_='forum_topic_pinned_notice forum_newtopic_box')) == 1: 
        pass
    
    else:
        data=html.find_all('div',class_="forum_op")
        for each in data:
            result=each.find(class_='content').get_text()

            return result


def get_reply_num(html):
    if len(html.find_all('div', class_='forum_topic_pinned_notice forum_newtopic_box')) == 1: 
        pass
    
    else:    
        replying=html.find_all('div', class_='topicstats')
        for each in replying:
            reply=each.find(class_='topicstats_value').get_text()
            if '@' in reply:
                continue
            else:
                #print(reply)
                return reply

def get_topic_post_time(html): 
    if len(html.find_all('div', class_='forum_topic_pinned_notice forum_newtopic_box')) == 1: 
        pass
    
    else:    
        table=html.find_all('span', class_='date')
        for each in table:
            result=each['data-timestamp']
            #print(each['data-timestamp'])

            return result

def access_author_profile(html):
    
    if len(html.find_all('div', class_='forum_topic_pinned_notice forum_newtopic_box')) == 1: 
        pass
    
    else:
        author_country=html.find_all('div', class_="popup_block_new forum_author_menu")
        #print(author_country)
    
        for each in author_country:
            if each.find(class_='popup_menu_item tight').get_text() == "View Profile":
                user_profile_url=each.find('a', class_='popup_menu_item tight').get('href', None)
                print('----------------')

                profile = request.Request(user_profile_url)

                opener=urllib.request.build_opener(urllib.request.HTTPHandler)
                urllib.request.install_opener(opener)
                profile.add_header('User-Agent','Mozilla/5.0 (Windows NT 6.1; \
                    WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36')

                html = request.urlopen(profile).read()

                soup1 = BeautifulSoup(html,'html.parser')
                
                
                if soup1.find('div', class_='header_real_name ellipsis') is None:
                    continue 
                else:
                    user_country=soup1.find('div', class_='header_real_name ellipsis').get_text()
                    return user_country
                
def get_herf_link(html):
    posted_time=[]
    post_time=html.find_all('a', class_="forum_topic_overlay")
    #print(post_time)
    
    for each in post_time:
        second_link=each.get('href', None)
        load_next_page(second_link)
        
                
def load_next_page(url):
    global data_retrieve
    page = request.Request(url)
    #proxy_support=urllib.request.ProxyHandler({'https':'10.172.49.230:3128'})
    opener=urllib.request.build_opener(urllib.request.HTTPHandler)
    urllib.request.install_opener(opener)
    page.add_header('User-Agent','Mozilla/5.0 (Windows NT 6.1; \
        WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36')

    html = request.urlopen(page).read()

    soup = BeautifulSoup(html,'html.parser')
    print('------------- Loading topic --------------' )

    data=[get_topic_name(soup),
         get_topic_content(soup),
         get_topic_post_time(soup),
         get_reply_num(soup),
         access_author_profile(soup)]
    data_retrieve.append(data)
    
    #get_topic_name(soup)
    #get_topic_content(soup)
    #get_topic_post_time(soup)
    #get_reply_num(soup)
    #access_author_profile(soup)

def save_to_csv(data):
    with open(r"D:\dataquest\PUBG\PUBG_analyzation1.csv",'w',newline='', encoding='utf-8') as csvFile:
        csvFile.write('\xEF\xBB\xBF') 
        writer =  csv.writer(csvFile)
        writer.writerow(('Topic_title','Posting_time', 'Content','Reply_num', 'Country')) 
        for each in data:
            writer.writerow(each)

if __name__ == '__main__':
    Url = "http://steamcommunity.com/app/578080/discussions/?fp="
    startPage = 1
    endPage = 123
    tieba_spider(Url, startPage, endPage)
    
    print("-------------------Complete--------------------")
    
